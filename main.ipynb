{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import ollama\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- LangChain imports ---\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema import LLMResult, Generation\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from pydantic import Field\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "# -------------------------------\n",
    "# Custom Ollama LLM using LangChain (Pydantic style)\n",
    "# -------------------------------\n",
    "class OllamaLLM(BaseLLM):\n",
    "    model: str = Field(...)\n",
    "    temperature: float = Field(default=0)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": self.temperature}\n",
    "        )\n",
    "        raw_output = \"\"\n",
    "        if \"message\" in response and \"content\" in response[\"message\"]:\n",
    "            raw_output = response[\"message\"][\"content\"].strip()\n",
    "        return raw_output\n",
    "\n",
    "    def _generate(self, prompts, stop=None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            text = self._call(prompt, stop=stop)\n",
    "            generation = Generation(text=text)\n",
    "            generations.append([generation])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop=None) -> str:\n",
    "        raise NotImplementedError(\"Async call not implemented for OllamaLLM.\")\n",
    "# -------------------------------\n",
    "# Define the expected output schema using ResponseSchema.\n",
    "# -------------------------------\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"station\", description=\"The brand or name of the gas station, inferred full name if possible; else null\"),\n",
    "    ResponseSchema(name=\"intersection\", description=\"The road, intersection, or address mentioned; else null\"),\n",
    "    ResponseSchema(name=\"gps\", description=\"GPS coordinates in 'lat,lon' format if inferable; else null\"),\n",
    "    ResponseSchema(name=\"line_flag\", description=\"Boolean: true if the post indicates a queue; else false\"),\n",
    "    ResponseSchema(name=\"oil_truck_flag\", description=\"Boolean: true if an oil truck is mentioned; else false\"),\n",
    "    ResponseSchema(name=\"status\", description=\"Event status: 'ended' if updated/comment indicates change; else 'unknown'\"),\n",
    "    ResponseSchema(name=\"gas_price\", description=\"Per-unit gas price (or inferred via division); if not determinable, 'unknown'\"),\n",
    "    ResponseSchema(name=\"time_offset_seconds\", description=\"Time difference (in seconds) from now to post time; e.g., 36000 for '10小時', else -1\")\n",
    "]\n",
    "\n",
    "text_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# A subclass that accepts an image file (a list of file paths)\n",
    "class OllamaLLMWithImages(OllamaLLM):\n",
    "    def _call(self, prompt: str, stop=None, images: list = None) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if images:\n",
    "            messages[0][\"images\"] = images\n",
    "        response = ollama.chat(model=self.model, messages=messages, options={\"temperature\": self.temperature})\n",
    "        raw_output = \"\"\n",
    "        if \"message\" in response and \"content\" in response[\"message\"]:\n",
    "            raw_output = response[\"message\"][\"content\"].strip()\n",
    "        return raw_output\n",
    "\n",
    "    def _generate(self, prompts, stop=None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            text = self._call(prompt, stop=stop)\n",
    "            generation = Generation(text=text)\n",
    "            generations.append([generation])\n",
    "        return LLMResult(generations=generations)\n",
    "    \n",
    "# -------------------------------\n",
    "# LangChain Prompt Template and Chain\n",
    "# -------------------------------\n",
    "template = \"\"\"You are an expert tasked with extracting gas station information from social media posts.\n",
    "You must respond with a JSON object ONLY, with no additional commentary or markdown formatting.\n",
    "\n",
    "the general post structure is as follows the first post is the original post and the second post and later posts are  comment on the original post for extra information or updates:\n",
    "``匿名成員 = name of the poster\n",
    "12小時\n",
    " \n",
    " · \n",
    "Petro Bovaird and Mississauga Rd. Was good at 9pm = text of the post\n",
    "所有心情：\n",
    "3\n",
    "3\n",
    "1 個回應  = number of likes and comments\n",
    "讚好\n",
    "回應\n",
    "傳送\n",
    "Rai Quan = name of the commenter (if any) \n",
    "Not good = comment (update)\n",
    "11小時 \n",
    "讚好\n",
    "回覆\n",
    "``\n",
    "Extract the following fields:\n",
    "- \"station\": The brand or name of the gas station (if mentioned), sometimes not just extract the name but to infer the full name (eg petro -> petro canada); else null.\n",
    "- \"intersection\": The road or intersection mentioned (e.g. an address or intersection :(e.g. an address : Bovaird and Mississauga Rd ) ); if not mentioned, null.\n",
    "- \"gps\": If GPS coordinates (formatted as \"lat,lon\") can be inferred from the intersection, return them; otherwise null.\n",
    "- \"line_flag\": true if the post indicates there is a queue/line; otherwise false.\n",
    "- \"oil_truck_flag\": true if the post mentions an oil truck (truck) is present; otherwise false.\n",
    "- \"status\": If the text indicates that the event has just begun (e.g. \"the staff just put on the sticker\") make it \"On-going\" or is updated by comments showing a change (e.g. \"back to normal\" or \"out of gas\" or \"no gas available\" or \"ended\" or \"not good\" etc.), return \"Ended\"; otherwise \"unknown\".\n",
    "- \"gas_price\": The per-unit gas price as seen in the image or from the post (94 for 1xx.xx or 1xx.xx, in this context 94 means premium gas / 94 octane). You are allowed to use the regular price as output or to divide the total price over filled valume. If not directly visible or inferable, return \"unknown\".\n",
    "- \"time_offset_seconds\": The time difference between now and the post time, expressed in seconds. For example, if the post says \"10小時\", output 36000, \"5小時\" output 18000 etc. If it cannot be determined, output -1.\n",
    "\n",
    "Input details:\n",
    "- Post text: \"{post_text}\"\n",
    "\n",
    "Please return ONLY a valid JSON object with the fields described above.\n",
    "\"\"\"\n",
    "\n",
    "text_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"post_text\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# # Create the chain using our custom OllamaLLM (using model \"deepseek-r1:7b\" in this example)\n",
    "# ollama_llm = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0)\n",
    "# chain = LLMChain(llm=ollama_llm, prompt=prompt_template)\n",
    "\n",
    "# Vision (price extraction) prompt template.\n",
    "price_template = \"\"\"You are an expert tasked with extracting the gas price from an image of a gas pump.\n",
    "You must respond with a JSON object ONLY, with no additional commentary or markdown formatting.\n",
    "\n",
    "Extract the following field:\n",
    "- \"gas_price\": The per-unit gas price as a number. If not determinable, return \"unknown\".\n",
    "\n",
    "Input details:\n",
    "- The image is provided as input.\n",
    "\n",
    "Please return ONLY a valid JSON object.\n",
    "\"\"\"\n",
    "\n",
    "price_prompt_template = PromptTemplate(\n",
    "    input_variables=[],  # no textual variable needed; the image is provided via the LLM call.\n",
    "    template=price_template,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Create two LangChain chains.\n",
    "# -------------------------------\n",
    "\n",
    "# Text analysis chain using deepseek model.\n",
    "deepseek_llm = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0)\n",
    "text_chain = LLMChain(llm=deepseek_llm, prompt=text_prompt_template)\n",
    "\n",
    "# Price extraction chain using vision model.\n",
    "vision_llm = OllamaLLMWithImages(model=\"llama3.2-vision\", temperature=0)\n",
    "price_chain = LLMChain(llm=vision_llm, prompt=price_prompt_template)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions (existing)\n",
    "# -------------------------------\n",
    "def human_delay(a=2, b=4):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "def download_image(image_url):\n",
    "    \"\"\"Download an image from image_url to a temporary file and return its file path.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n",
    "            temp_file.write(response.content)\n",
    "            temp_file.close()\n",
    "            return temp_file.name\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading image:\", e)\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# Analysis Function: Combined analysis using text and vision.\n",
    "# -------------------------------\n",
    "\n",
    "def analyze_post_combined(text, image_url):\n",
    "    # Run text analysis.\n",
    "    text_result_raw = text_chain.run(post_text=text)\n",
    "    print(\"\\n=== Raw Deepseek (Text) Output ===\")\n",
    "    print(text_result_raw)\n",
    "    try:\n",
    "        text_result = text_output_parser.parse(text_result_raw)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing text output:\", e)\n",
    "        text_result = {}\n",
    "    \n",
    "    # # If an image URL is provided, download it and run vision analysis.\n",
    "    # if image_url:\n",
    "    #     local_image_path = download_image(image_url)\n",
    "    #     if local_image_path:\n",
    "    #         # Call the vision chain, passing the image via the \"images\" argument.\n",
    "    #         # Our custom LLM subclass accepts an extra 'images' parameter.\n",
    "    #         gc.collect()\n",
    "    #         price_result_raw = vision_llm._call(prompt=price_prompt_template.format(), images=[local_image_path])\n",
    "    #         print(\"\\n=== Raw Vision (Price) Output ===\")\n",
    "    #         print(price_result_raw)\n",
    "    #         try:\n",
    "    #             price_result = json.loads(price_result_raw)\n",
    "    #         except Exception as e:\n",
    "    #             print(\"Error parsing price output:\", e)\n",
    "    #             price_result = {}\n",
    "    #         # Merge the gas_price from vision output into text_result.\n",
    "    #         if \"gas_price\" in price_result:\n",
    "    #             text_result[\"gas_price\"] = price_result[\"gas_price\"]\n",
    "    #         # Clean up the downloaded image.\n",
    "    #         os.remove(local_image_path)\n",
    "    \n",
    "    # Compute a timestamp field using time_offset_seconds.\n",
    "    utc_now = datetime.utcnow()\n",
    "    try:\n",
    "        offset = float(text_result.get(\"time_offset_seconds\", -1))\n",
    "    except Exception:\n",
    "        offset = -1\n",
    "    if offset == -1:\n",
    "        text_result[\"timestamp\"] = utc_now.isoformat() + \"Z\"\n",
    "    else:\n",
    "        text_result[\"timestamp\"] = (utc_now - timedelta(seconds=offset)).isoformat() + \"Z\"\n",
    "    \n",
    "    return text_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import ollama\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def human_delay(a=2, b=4):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "def download_image(image_url):\n",
    "    \"\"\"Download an image from image_url to a temporary file and return its file path.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n",
    "            temp_file.write(response.content)\n",
    "            temp_file.close()\n",
    "            return temp_file.name\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading image:\", e)\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# Analysis Function using Ollama with English Prompt\n",
    "# -------------------------------\n",
    "\n",
    "def analyze_post(text, image_url):\n",
    "    \"\"\"\n",
    "    Use Ollama's Llama-3.2-Vision model to analyze the post.\n",
    "    The prompt instructs the model (in English) to extract:\n",
    "      - station: The brand or name of the gas station (if mentioned), sometimes not just extract the name but to infer the full name (eg petro -> petro canada); else null.\n",
    "      - intersection: The road or intersection or postcode mentioned (e.g. an address : Bovaird and Mississauga Rd ) ; else null.\n",
    "      - gps: If GPS coordinates (formatted as \"lat,lon\") can be inferred from the intersection, return them; else null.\n",
    "      - line_flag: true if the post indicates there is a queue/line; otherwise false.\n",
    "      - oil_truck_flag: true if the post mentions an oil truck is present; otherwise false.\n",
    "      - time_since_start: \"current\" if the event has just begun or updated comments indicate a recent change; otherwise a time offset or \"unknown\".\n",
    "      - gas_price: The per-unit gas price as seen in the image; if not visible or inferable, \"unknown\".\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert tasked with extracting gas station information from social media posts.\n",
    "You must respond with a JSON object ONLY, with no additional commentary or markdown formatting.\n",
    "\n",
    "the general post structure is as follows the first post is the original post and the second post and later posts are  comment on the original post for extra information or updates:\n",
    "``匿名成員 = name of the poster\n",
    "12小時\n",
    " \n",
    " · \n",
    "Petro Bovaird and Mississauga Rd. Was good at 9pm = text of the post\n",
    "所有心情：\n",
    "3\n",
    "3\n",
    "1 個回應  = number of likes and comments\n",
    "讚好\n",
    "回應\n",
    "傳送\n",
    "Rai Quan = name of the commenter (if any) \n",
    "Not good = comment (update)\n",
    "11小時 \n",
    "讚好\n",
    "回覆\n",
    "``\n",
    "Extract the following fields:\n",
    "- \"station\": The brand or name of the gas station (if mentioned), sometimes not just extract the name but to infer the full name (eg petro -> petro canada); else null.\n",
    "- \"intersection\": The road or intersection mentioned (e.g. an address or intersection :(e.g. an address : Bovaird and Mississauga Rd ) ); if not mentioned, null.\n",
    "- \"gps\": If GPS coordinates (formatted as \"lat,lon\") can be inferred from the intersection, return them; otherwise null.\n",
    "- \"line_flag\": true if the post indicates there is a queue/line; otherwise false.\n",
    "- \"oil_truck_flag\": true if the post mentions an oil truck is present; otherwise false.\n",
    "- \"status\": If the text indicates that the event has just begun (e.g. \"the staff just put on the sticker\") or is updated by comments showing a change (e.g. \"back to normal\" or \"out of gas\" or \"no gas available\" or \"ended\" or \"not good\" etc.), return \"ended\"; otherwise \"unknown\".\n",
    "- \"gas_price\": The per-unit gas price as seen in the image or from the post. You are allowed to use the regular price as output or to divide the total price over filled valume. If not directly visible or inferable, return \"unknown\".\n",
    "- \"time_offset_seconds\": The time difference between now and the post time, expressed in seconds. For example, if the post says \"10小時\", output 36000, \"5小時\" output 18000 etc. If it cannot be determined, output -1.\n",
    "\n",
    "Input details:\n",
    "- Post text: \"{text}\"\n",
    "- Image: Provided below (if available).\n",
    "\n",
    "Please return ONLY a valid JSON object with the fields described above.\n",
    "\"\"\"\n",
    "    # Print out the input prompt for debugging.\n",
    "    # print(\"\\n=== Input Prompt to Ollama ===\")\n",
    "    # print(prompt)\n",
    "    \n",
    "    # Download image locally if an image URL is provided.\n",
    "    local_image_path = None\n",
    "    if image_url:\n",
    "        local_image_path = download_image(image_url)\n",
    "\n",
    "    try:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }]\n",
    "        if local_image_path:\n",
    "            messages[0][\"images\"] = [local_image_path]\n",
    "        \n",
    "        # response = ollama.chat(model=\"llama3.2-vision\", messages=messages, options={\"temperature\": 0})\n",
    "        response = ollama.chat(model=\"deepseek-r1:7b\", messages=messages, options={\"temperature\": 0})\n",
    "        # Debug: print the raw response from Ollama.\n",
    "        print(\"\\n=== Raw Ollama Response ===\")\n",
    "        print(response)\n",
    "        \n",
    "        # Now, retrieve the JSON string from response.message.content.\n",
    "        raw_output = \"\"\n",
    "        if \"message\" in response and \"content\" in response[\"message\"]:\n",
    "            raw_output = response[\"message\"][\"content\"].strip()\n",
    "        \n",
    "        if not raw_output:\n",
    "            print(\"Empty output from Ollama model.\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(raw_output)\n",
    "        except json.JSONDecodeError as je:\n",
    "            print(\"JSON decoding error:\", je)\n",
    "            print(\"Model output was:\")\n",
    "            print(raw_output)\n",
    "            result = {}\n",
    "    except Exception as e:\n",
    "        print(\"Error calling or parsing response from Ollama model:\", e)\n",
    "        result = {}\n",
    "    finally:\n",
    "        if local_image_path and os.path.exists(local_image_path):\n",
    "            os.remove(local_image_path)\n",
    "    # Add timestamp field using time_offset_seconds.\n",
    "    utc_now = datetime.utcnow()\n",
    "    try:\n",
    "        offset = float(result.get(\"time_offset_seconds\", -1))\n",
    "    except Exception:\n",
    "        offset = -1\n",
    "    if offset == -1:\n",
    "        result[\"timestamp\"] = utc_now.isoformat() + \"Z\"\n",
    "    else:\n",
    "        result[\"timestamp\"] = (utc_now - timedelta(seconds=offset)).isoformat() + \"Z\"\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configuration: Facebook group URL and cookies file.\n",
    "GROUP_URL = \"https://www.facebook.com/groups/1982935245273808/?sorting_setting=CHRONOLOGICAL\"\n",
    "COOKIES_FILE = \"fb_cookies.pkl\"\n",
    "\n",
    "# A queue for image URLs (for later processing)\n",
    "image_queue = []\n",
    "\n",
    "# -------------------------------\n",
    "# Main Scraping Code: Collect Posts and Batch Process\n",
    "# -------------------------------\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "# Uncomment for headless mode:\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "posts_data = []\n",
    "\n",
    "try:\n",
    "    # 1. Navigate to Facebook to set the cookie domain.\n",
    "    driver.get(\"https://www.facebook.com/\")\n",
    "    human_delay(2, 3)\n",
    "\n",
    "    # 2. Load cookies if available.\n",
    "    if os.path.exists(COOKIES_FILE):\n",
    "        print(\"Loading cookies...\")\n",
    "        with open(COOKIES_FILE, \"rb\") as f:\n",
    "            cookies = pickle.load(f)\n",
    "        for cookie in cookies:\n",
    "            if 'sameSite' in cookie and cookie['sameSite'] == 'None':\n",
    "                cookie['sameSite'] = 'Strict'\n",
    "            try:\n",
    "                driver.add_cookie(cookie)\n",
    "            except Exception as e:\n",
    "                print(\"Error adding cookie:\", e)\n",
    "        driver.refresh()\n",
    "        human_delay(3, 5)\n",
    "    else:\n",
    "        print(\"No cookies found. Logging in manually...\")\n",
    "        email_input = driver.find_element(By.ID, \"email\")\n",
    "        password_input = driver.find_element(By.ID, \"pass\")\n",
    "        email_input.send_keys(FB_EMAIL)\n",
    "        human_delay(1, 2)\n",
    "        password_input.send_keys(FB_PASSWORD)\n",
    "        human_delay(1, 2)\n",
    "        password_input.send_keys(Keys.RETURN)\n",
    "        human_delay(5, 7)\n",
    "        input(\"Complete any 2FA in the browser, then press Enter to continue...\")\n",
    "        cookies = driver.get_cookies()\n",
    "        with open(COOKIES_FILE, \"wb\") as f:\n",
    "            pickle.dump(cookies, f)\n",
    "        print(\"Cookies saved for future sessions.\")\n",
    "\n",
    "    # 3. Navigate to the target Facebook group.\n",
    "    driver.get(GROUP_URL)\n",
    "    human_delay(5, 7)\n",
    "\n",
    "    # 4. Slowly scroll to load more posts.\n",
    "    for _ in range(3):\n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_increment = random.randint(300, 800)\n",
    "        for pos in range(0, current_height, scroll_increment):\n",
    "            driver.execute_script(\"window.scrollTo(0, arguments[0]);\", pos)\n",
    "            human_delay(0.2, 0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        human_delay(3, 5)\n",
    "\n",
    "    # 5. Locate post containers.\n",
    "    # Here we exclude containers whose class attribute contains \"xqcrz7y\" (a typical marker for comment posts).\n",
    "    posts = driver.find_elements(By.XPATH, \"//div[@role='article' and not(contains(@class, 'xqcrz7y'))]\")\n",
    "    print(f\"Found {len(posts)} posts.\")\n",
    "\n",
    "    # For development, collect only the first 10 posts.\n",
    "    # posts = posts[:10]\n",
    "    print(f\"Collecting only the first {len(posts)} posts for development.\")\n",
    "\n",
    "    # 6. Extract data from each post.\n",
    "    for i in range(len(posts)):\n",
    "        attempts = 0\n",
    "        post_info = {\"text\": \"\", \"image_urls\": [], \"post_time\": \"\"}\n",
    "        while attempts < 3:\n",
    "            try:\n",
    "                # Re-fetch posts (to avoid stale element issues) using the filtered XPath.\n",
    "                posts = driver.find_elements(By.XPATH, \"//div[@role='article' and not(contains(@class, 'xqcrz7y'))]\")\n",
    "                post = posts[i]\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", post)\n",
    "                human_delay(1, 2)\n",
    "\n",
    "                # Expand \"See More\" if available.\n",
    "                try:\n",
    "                    more_links = post.find_elements(By.XPATH, \".//a[contains(text(), 'See More')]\")\n",
    "                    if more_links:\n",
    "                        actions.move_to_element(more_links[0]).perform()\n",
    "                        human_delay(0.5, 1)\n",
    "                        driver.execute_script(\"arguments[0].click();\", more_links[0])\n",
    "                        human_delay(2, 3)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error clicking 'See More' in post {i+1}: {e}\")\n",
    "\n",
    "                # Get post text.\n",
    "                post_text = driver.execute_script(\"return arguments[0].innerText;\", post)\n",
    "                post_info[\"text\"] = post_text.strip() if post_text.strip() else \"\"\n",
    "        \n",
    "\n",
    "                # Extract image URLs.\n",
    "                try:\n",
    "                    img_tags = post.find_elements(By.XPATH, \".//img\")\n",
    "                    image_urls = []\n",
    "                    if img_tags:\n",
    "                        for img in img_tags:\n",
    "                            src = img.get_attribute(\"src\")\n",
    "                            if src and src not in image_queue:\n",
    "                                image_queue.append(src)\n",
    "                                image_urls.append(src)\n",
    "                    post_info[\"image_urls\"] = image_urls\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting images in post {i+1}: {e}\")\n",
    "\n",
    "                # Debug output.\n",
    "                inner_html = post.get_attribute(\"innerHTML\")\n",
    "                print(f\"\\n=== Post {i+1} ===\")\n",
    "                print(\"Post text:\")\n",
    "                print(post_info[\"text\"] if post_info[\"text\"] else \"(No text found)\")\n",
    "                print(\"Inner HTML snippet:\")\n",
    "                print(inner_html[:300] + \"...\" if len(inner_html) > 300 else inner_html)\n",
    "                break  # Successfully extracted post info.\n",
    "            except StaleElementReferenceException:\n",
    "                print(f\"StaleElementReferenceException encountered in post {i+1}. Retrying...\")\n",
    "                human_delay(1, 2)\n",
    "                attempts += 1\n",
    "        else:\n",
    "            print(f\"Failed to process post {i+1} after several retries.\")\n",
    "        \n",
    "\n",
    "        # Apply a simple heuristic: if the post text is very short or lacks a separator (\" · \"),\n",
    "        # it's likely a comment update rather than an original post. Skip these.\n",
    "        if len(post_info[\"text\"]) < 100 or \" · \" not in post_info[\"text\"]:\n",
    "            print(f\"Skipping post {i+1} (likely a duplicate comment update).\")\n",
    "        else:\n",
    "            posts_data.append(post_info)\n",
    "\n",
    "    print(\"\\n=== Finished collecting post data ===\")\n",
    "    print(f\"Total posts collected: {len(posts_data)}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Batch Process Posts using Ollama\n",
    "    # -------------------------------\n",
    "    print(\"\\n=== Batch processing posts (using Ollama) ===\")\n",
    "    analysis_results = []\n",
    "    for idx, post in enumerate(posts_data, start=1):\n",
    "        # For image analysis, take the first image URL if available.\n",
    "        image_url = post[\"image_urls\"][0] if post[\"image_urls\"] else None\n",
    "        # analysis = analyze_post(post[\"text\"], image_url)\n",
    "        analysis = analyze_post_combined(post[\"text\"],image_url)\n",
    "        analysis_results.append(analysis)\n",
    "        print(f\"\\n=== Analysis result for post {idx} ===\")\n",
    "        print(json.dumps(analysis, indent=2, ensure_ascii=False))\n",
    "        human_delay(2, 4)\n",
    "\n",
    "    # Optionally, display the full image queue.\n",
    "    print(\"\\n=== Image Queue (for later processing) ===\")\n",
    "    for idx, url in enumerate(image_queue, start=1):\n",
    "        print(f\"{idx}. {url}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'station': 'Petro Canada',\n",
       "  'intersection': 'Bovaird and Mississauga Rd',\n",
       "  'gps': None,\n",
       "  'line_flag': False,\n",
       "  'oil_truck_flag': False,\n",
       "  'status': 'ended',\n",
       "  'gas_price': '94 for 143.9',\n",
       "  'time_offset_seconds': 108000,\n",
       "  'timestamp': '2025-02-27T00:58:30.039157Z'},\n",
       " {'station': 'Petro Canada',\n",
       "  'intersection': 'Bovaird and Mississauga Rd',\n",
       "  'gps': None,\n",
       "  'line_flag': False,\n",
       "  'oil_truck_flag': False,\n",
       "  'status': 'ended',\n",
       "  'gas_price': 'unknown',\n",
       "  'time_offset_seconds': 54000,\n",
       "  'timestamp': '2025-02-27T15:59:00.619116Z'},\n",
       " {'station': 'Petro Canada',\n",
       "  'intersection': 'Sandalwood and Mississauga Rd',\n",
       "  'gps': '43.6837,-79.9345',\n",
       "  'line_flag': False,\n",
       "  'oil_truck_flag': False,\n",
       "  'status': 'ended',\n",
       "  'gas_price': 94,\n",
       "  'time_offset_seconds': 43200,\n",
       "  'timestamp': '2025-02-27T18:59:25.446922Z'},\n",
       " {'station': None,\n",
       "  'intersection': 'St Clair and Kennedy',\n",
       "  'gps': None,\n",
       "  'line_flag': False,\n",
       "  'oil_truck_flag': False,\n",
       "  'status': 'unknown',\n",
       "  'gas_price': '135.9',\n",
       "  'time_offset_seconds': 57600,\n",
       "  'timestamp': '2025-02-27T14:59:50.336187Z'},\n",
       " {'station': 'Petro',\n",
       "  'intersection': 'Bovaird and Mississauga Rd',\n",
       "  'gps': None,\n",
       "  'line_flag': False,\n",
       "  'oil_truck_flag': False,\n",
       "  'status': 'ended',\n",
       "  'gas_price': 'unknown',\n",
       "  'time_offset_seconds': -1,\n",
       "  'timestamp': '2025-02-28T07:00:10.466585Z'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_link  = \"https://scontent-yyz1-1.xx.fbcdn.net/v/t39.30808-6/481478506_10233214677856295_4449808216704385674_n.jpg?stp=cp6_dst-jpegr_p526x296_tt6&_nc_cat=110&ccb=1-7&_nc_sid=aa7b47&_nc_ohc=KczbmWSByakQ7kNvgHFlwSF&_nc_oc=AdiynByPy9TZM0pCpN6pJ3C_zhM73O5sRut_oGXIs9wqZMDhDqIBe2hA8J06-e53JzI&_nc_zt=23&se=-1&_nc_ht=scontent-yyz1-1.xx&_nc_gid=AttlxX2sc-S58_aDQQsj5Wo&oh=00_AYCaeB-ajBhDu7bssX98KZ3Y-GBtF5wAScnIv66azJs9jg&oe=67C3FC47\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_text = \"\"\"匿名成員\n",
    "12小時\n",
    " \n",
    " · \n",
    "Petro Bovaird and Mississauga Rd. Was good at 9pm\n",
    "所有心情：\n",
    "3\n",
    "3\n",
    "1 個回應\n",
    "讚好\n",
    "回應\n",
    "傳送\n",
    "Rai Quan\n",
    "Not good\n",
    "11小時\n",
    "讚好\n",
    "回覆\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "以 Kevin Lam 的身分回應\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Raw Deepseek (Text) Output ===\n",
      "<think>\n",
      "嗯，我现在需要处理这个任务，提取气站信息。首先，我得仔细阅读用户提供的社交媒体帖子内容，然后按照要求从中提取相关字段。\n",
      "\n",
      "首先，看看原帖的内容。匿名成員发了一个帖子，说Petro Bovaird和Mississauga Rd在9点好，所有心情有3、3、1个回应，之后还有评论来自Rai Quan，说“Not good”。时间显示12小时后，然后又是11小时。\n",
      "\n",
      "接下来，我需要提取的字段包括：station、intersection、gps、line_flag、oil_truck_flag、status、gas_price和time_offset_seconds。\n",
      "\n",
      "首先，station。原帖提到Petro Bovaird，看起来像是气站的品牌，但可能完整名称是Petro Canada，所以这里填“Petro Canada”。\n",
      "\n",
      "然后是intersection。地址是Bovaird和Mississauga Rd，这应该就是交汇处，所以填这个字符串。\n",
      "\n",
      "接下来是gps。如果有交汇处，是否能推断出坐标？通常情况下，可能需要更多信息才能确定具体的经纬度，但这里没有提供，所以gps设为null。\n",
      "\n",
      "line_flag。帖子里提到“Was good at 9pm”，但后来评论说“Not good”。这说明情况发生了变化，所以可能有排队。所以line_flag设为true。\n",
      "\n",
      "oil_truck_flag。原帖中没有提到油卡或运输车辆，所以设为false。\n",
      "\n",
      "status。评论说“Not good”，这表明情况已经改变，可能结束了，所以status设为“ended”。\n",
      "\n",
      "gas_price。原帖里没有直接提到价格，所以设为unknown。\n",
      "\n",
      "time_offset_seconds。帖子显示12小时后和11小时，这可能是指发帖后的时间。但需要确认是否是相对于当前的时间还是发帖时的时间。假设现在是某个时间点，发帖时间是t，那么“12小時”表示现在比t晚了12小时，所以time_offset_seconds为12*3600=43200秒。\n",
      "\n",
      "综上所述，我需要把这些信息整理成一个JSON对象。\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"station\": \"Petro Canada\",\n",
      "  \"intersection\": \"Bovaird and Mississauga Rd\",\n",
      "  \"gps\": null,\n",
      "  \"line_flag\": true,\n",
      "  \"oil_truck_flag\": false,\n",
      "  \"status\": \"ended\",\n",
      "  \"gas_price\": \"unknown\",\n",
      "  \"time_offset_seconds\": 43200\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "POST predict: Post \"http://127.0.0.1:3888/completion\": read tcp 127.0.0.1:3890->127.0.0.1:3888: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m analyze_post_combined(post_text, image_link)\n",
      "Cell \u001b[1;32mIn[25], line 215\u001b[0m, in \u001b[0;36manalyze_post_combined\u001b[1;34m(text, image_url)\u001b[0m\n\u001b[0;32m    211\u001b[0m local_image_path \u001b[38;5;241m=\u001b[39m download_image(image_url)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_image_path:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Call the vision chain, passing the image via the \"images\" argument.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Our custom LLM subclass accepts an extra 'images' parameter.\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     price_result_raw \u001b[38;5;241m=\u001b[39m vision_llm\u001b[38;5;241m.\u001b[39m_call(prompt\u001b[38;5;241m=\u001b[39mprice_prompt_template\u001b[38;5;241m.\u001b[39mformat(), images\u001b[38;5;241m=\u001b[39m[local_image_path])\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Raw Vision (Price) Output ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(price_result_raw)\n",
      "Cell \u001b[1;32mIn[25], line 79\u001b[0m, in \u001b[0;36mOllamaLLMWithImages._call\u001b[1;34m(self, prompt, stop, images)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images:\n\u001b[0;32m     78\u001b[0m     messages[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39mmessages, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature})\n\u001b[0;32m     80\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\Documents\\Project\\gas\\.conda\\Lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    334\u001b[0m     ChatResponse,\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    337\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    338\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    339\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    340\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    341\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    342\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    343\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    344\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    345\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    346\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    347\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\Documents\\Project\\gas\\.conda\\Lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\Documents\\Project\\gas\\.conda\\Lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: POST predict: Post \"http://127.0.0.1:3888/completion\": read tcp 127.0.0.1:3890->127.0.0.1:3888: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)"
     ]
    }
   ],
   "source": [
    "analysis = analyze_post_combined(post_text, image_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'station': 'Petro Canada',\n",
       " 'intersection': 'Bovaird and Mississauga Rd',\n",
       " 'gps': None,\n",
       " 'line_flag': False,\n",
       " 'oil_truck_flag': False,\n",
       " 'status': 'ended',\n",
       " 'gas_price': '94 for 143.9',\n",
       " 'time_offset_seconds': 108000,\n",
       " 'timestamp': '2025-02-27T00:52:31.359069Z'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
